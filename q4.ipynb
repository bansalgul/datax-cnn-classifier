{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bing_image_downloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbing_image_downloader\u001b[39;00m \u001b[39mimport\u001b[39;00m downloader\n\u001b[1;32m      2\u001b[0m downloader\u001b[39m.\u001b[39mdownload(\u001b[39m\"\u001b[39m\u001b[39mManga Comic Strips\u001b[39m\u001b[39m\"\u001b[39m, limit\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,  output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmanga_comics/\u001b[39m\u001b[39m'\u001b[39m, adult_filter_off\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, force_replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, timeout\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bing_image_downloader'"
     ]
    }
   ],
   "source": [
    "from bing_image_downloader import downloader\n",
    "downloader.download(\"Manga Comic Strips\", limit=100,  output_dir='manga_comics/', adult_filter_off=True, force_replace=False, timeout=600, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images from page: https://www.imdb.com/title/tt2560140/mediaindex?page=1&ref_=ttmi_mi_sm\n",
      "Saved image: img0.jpg\n",
      "Saved image: img1.jpg\n",
      "Saved image: img2.jpg\n",
      "Saved image: img3.jpg\n",
      "Saved image: img4.jpg\n",
      "Saved image: img5.jpg\n",
      "Saved image: img6.jpg\n",
      "Saved image: img7.jpg\n",
      "Saved image: img8.jpg\n",
      "Saved image: img9.jpg\n",
      "Saved image: img10.jpg\n",
      "Saved image: img11.jpg\n",
      "Saved image: img12.jpg\n",
      "Saved image: img13.jpg\n",
      "Saved image: img14.jpg\n",
      "Saved image: img15.jpg\n",
      "Saved image: img16.jpg\n",
      "Saved image: img17.jpg\n",
      "Saved image: img18.jpg\n",
      "Saved image: img19.jpg\n",
      "Saved image: img20.jpg\n",
      "Saved image: img21.jpg\n",
      "Saved image: img22.jpg\n",
      "Saved image: img23.jpg\n",
      "Saved image: img24.jpg\n",
      "Saved image: img25.jpg\n",
      "Saved image: img26.jpg\n",
      "Saved image: img27.jpg\n",
      "Saved image: img28.jpg\n",
      "Saved image: img29.jpg\n",
      "Saved image: img30.jpg\n",
      "Saved image: img31.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "base_url = 'https://www.imdb.com/title/tt2560140/mediaindex?page={}&ref_=ttmi_mi_sm'\n",
    "img_urls = []\n",
    "\n",
    "def scrape_images(url):\n",
    "    raw = requests.get(url)\n",
    "    if raw.status_code == 200:\n",
    "        soup = BeautifulSoup(raw.content, 'html.parser')\n",
    "        images = soup.find_all('div', class_='media_index_thumb_list')   #gallery-image-wrapper\n",
    "        if images:\n",
    "            for div in images:\n",
    "                img_tags = div.find_all('img')  # Find all img tags within the div\n",
    "                for img_tag in img_tags:\n",
    "                    img_url = img_tag.get('src')  # Extracting the image URL\n",
    "                    if img_url:\n",
    "                        img_urls.append(img_url)\n",
    "        else:\n",
    "            print(\"No images found on page:\", url)\n",
    "    else:\n",
    "        print(\"Failed to fetch page:\", url)\n",
    "\n",
    "# Create a folder to store images\n",
    "folder_path = 'aot_images'  # Change the folder name here\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "for page_number in range(1, 2):  # Adjust the range according to your assumption\n",
    "    page_url = base_url.format(page_number)\n",
    "    print(\"Scraping images from page:\", page_url)\n",
    "    scrape_images(page_url)\n",
    "\n",
    "# Download images\n",
    "for idx, img_url in enumerate(img_urls):\n",
    "    # Make a request to the image URL\n",
    "    img_source = requests.get(img_url, stream=True)\n",
    "\n",
    "    # Save image to folder_path\n",
    "    with open(os.path.join(folder_path, f'img{idx}.jpg'), 'wb') as file:  # Save to the new folder\n",
    "        shutil.copyfileobj(img_source.raw, file)\n",
    "        print(f\"Saved image: img{idx}.jpg\")  # Print statement to show which image is being saved\n",
    "\n",
    "    del img_source  # Remove the file from memory\n",
    "\n",
    "    if idx > 30:  # Limiting to 200 images for demonstration\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images from page: http://pt.jikos.cz/garfield/1978/\n",
      "Scraping images from page: http://pt.jikos.cz/garfield/1979/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "# Example usage:\n",
    "url = \"http://pt.jikos.cz/garfield/{}/\"\n",
    "folder = \"garfield_comics\"\n",
    "\n",
    "def download_images_from_page(url, folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all <img> tags within the table\n",
    "    image_tags = soup.select('table img')\n",
    "    \n",
    "    # Extract image URLs and download images\n",
    "    for i, img_tag in enumerate(image_tags):\n",
    "        # Get the value of the 'src' attribute\n",
    "        img_url = img_tag.get('src')\n",
    "        if img_url:\n",
    "            # Construct absolute URL if the 'src' attribute is a relative URL\n",
    "            parsed_url = urlparse(url)\n",
    "            img_url = urljoin(parsed_url.scheme + '://' + parsed_url.netloc, img_url)\n",
    "            \n",
    "            # Download image\n",
    "            img_name = f\"{folder}_{i}.jpg\"\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                img_data = requests.get(img_url).content\n",
    "                f.write(img_data)\n",
    "\n",
    "\n",
    "for year in range(1978, 1980):  # Adjust the range according to your assumption\n",
    "    year_url = url.format(year)\n",
    "    print(\"Scraping images from page:\", year_url)\n",
    "    download_images_from_page(year_url, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new classification layers\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 13s 5s/step - loss: 1.9463 - accuracy: 0.4667 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 11s 4s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.2364e-06 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 12s 4s/step - loss: 5.4270e-07 - accuracy: 1.0000 - val_loss: 5.4674e-08 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 11s 4s/step - loss: 1.3126e-05 - accuracy: 1.0000 - val_loss: 2.2727e-07 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 11s 4s/step - loss: 4.8799e-05 - accuracy: 1.0000 - val_loss: 8.6266e-07 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 5.5230e-04 - accuracy: 1.0000 - val_loss: 1.9519e-06 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 8.6449e-04 - accuracy: 1.0000 - val_loss: 1.9591e-06 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 8.0616e-04 - accuracy: 1.0000 - val_loss: 1.2556e-06 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 4.8123e-04 - accuracy: 1.0000 - val_loss: 6.1872e-07 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 10s 4s/step - loss: 2.4353e-04 - accuracy: 1.0000 - val_loss: 2.7796e-07 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 2.7796e-07 - accuracy: 1.0000\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(val_images, val_labels)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed.\n",
      "Train images shape: (45, 224, 224, 3)\n",
      "Train labels shape: (45,)\n",
      "Validation images shape: (5, 224, 224, 3)\n",
      "Validation labels shape: (5,)\n",
      "Test images shape: (13, 224, 224, 3)\n",
      "Test labels shape: (13,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Define the image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(img_path):\n",
    "    img = load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = img_array.astype(\"float\") / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "# List to store image paths and corresponding labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Assuming you have directories named 'manga' and 'classical' containing images\n",
    "manga_dir = '/Users/gulbansal/Documents/afoexam/aot_images'\n",
    "classical_dir = '/Users/gulbansal/Documents/afoexam/garfield_comics'\n",
    "\n",
    "# Iterate over manga images\n",
    "for filename in os.listdir(manga_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        img_path = os.path.join(manga_dir, filename)\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(0)  # Folder name as label\n",
    "\n",
    "# Iterate over classical images\n",
    "for filename in os.listdir(classical_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        img_path = os.path.join(classical_dir, filename)\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(1)  # Folder name as label\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Initialize lists to store preprocessed images\n",
    "preprocessed_images = []\n",
    "\n",
    "# Preprocess images\n",
    "for img_path in image_paths:\n",
    "    preprocessed_img = preprocess_image(img_path)\n",
    "    preprocessed_images.append(preprocessed_img)\n",
    "\n",
    "# Convert preprocessed images to numpy array\n",
    "preprocessed_images = np.array(preprocessed_images)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(preprocessed_images, labels, test_size=0.2, random_state=42)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Data preprocessing completed.\")\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 286ms/step\n",
      "Predictions: [[0.76375127]]\n"
     ]
    }
   ],
   "source": [
    "# Load a random image\n",
    "random_img_path = '/Users/gulbansal/Documents/afoexam/naruto.jpg'  # Replace with the path to your random image\n",
    "random_img = preprocess_image(random_img_path)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(np.expand_dims(random_img, axis=0))\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Set a threshold\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert probability to binary class label using the threshold\n",
    "binary_prediction = 1 if predictions[0][0] > threshold else 0\n",
    "\n",
    "print(binary_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comic strips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define URLs to scrape\n",
    "manga_url = \"https://in.pinterest.com/emmaslavinhall/animemangacomic-strips/\"\n",
    "classical_url = \"https://in.pinterest.com/wegardentoo/vintage-comic-book-covers/\"\n",
    "\n",
    "# Create directories to store images\n",
    "os.makedirs(\"manga_images\", exist_ok=True)\n",
    "os.makedirs(\"classical_images\", exist_ok=True)\n",
    "\n",
    "# Function to download images from a URL\n",
    "def download_images(url, output_dir):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    image_tags = soup.find_all(\"img\")\n",
    "    for img_tag in image_tags:\n",
    "        img_url = img_tag.get(\"src\")\n",
    "        if img_url:\n",
    "            img_name = os.path.basename(img_url)\n",
    "            img_path = os.path.join(output_dir, img_name)\n",
    "            with open(img_path, \"wb\") as img_file:\n",
    "                img_file.write(requests.get(img_url).content)\n",
    "\n",
    "# Download images for Manga and Classical categories\n",
    "download_images(manga_url, \"manga_images\")\n",
    "download_images(classical_url, \"classical_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
